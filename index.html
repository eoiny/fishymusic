<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Rumble Fish</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="images/favicon.png">

</head>
<body>

    <!-- Hero image
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div id="hero-image">
  </div>
  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <div class="row">
      <div class="column" style="margin-top: 5%">
       <!-- <h4>Basic Page</h4>-->

       <p class="after-image"><span class="dropcap">I</span>n the liner notes for his album <i>Ambient 1: Music</i>, Brian Eno described a vision for ambient music that could engage listeners —providing an experience that is as compelling as it is unobtrusive. He wanted to "...accommodate many levels of listening attention without enforcing one in particular; it must be as ignorable as it is interesting." Prompted by a college assignment to "identify a novel user experience," I gravitated towards the infinite possibilities of generative music—an area where innovation converges with unpredictability. I have long been fascinated by the idea of "systems that make music". It's so interesting that ground breaking compositions like Terry Riley's "In C" or Steve Reich's "Piano Phase" can come down to a relatively simple set of rules. I began to wonder if I could design a generative music system that might incorporate a biological component. I was thinking specifically of goldfish - life in aquariums can often refelect the calm introspection of ambient soundscapes.</p>
      </div>
    
      
    
      
    </div>

   

       <div class="row">
        <div class="one-half column">
          <h4 class="after-image">A design process</h4>
          <p>The design process is iterative. In my experience as a desiger it typically it begins with a real problem that is experienced by a client, and requires a deep understanding of the problem and its context through research. This work in turn leads to a clear but abstract articulation of the challenge or root causes of the problem. This sets the stage for divergent thinking, where many potential ideas are generated as potential solutions. These ideas are then rapidly shaped into prototypes and subjected to testing, which provides further insights into the problem. This iterative cycle may run multiple times, gradually focussing in on a satisfactory solution. In practice, iterations often include smaller feedback loops—prototypes may spark new ideas or influence the original problem definition, leading to a fluid and responsive design process. 
            </p>
        </div>

        <div class="one-half column">
          <img class="u-max-full-width" src="images/process2.png" style="margin-top: 0%">
          <div class="caption">
            <p><i>Rumble Fish</i> started with an abstract idea to test.</p>
          </div>
  
        </div>
        
      </div>

      <div class="row">
        <h4 class="after-image">Skimming the surface: Initial idea</h4>
          
       </div>


     


      <div class="row">
        <div class="one-half column">
          <img class="u-max-full-width" src="images/grid1.png" style="margin-top: 5%">
          <div class="caption">
            <p><i><a href="https://gist.github.com/vasturiano/e70e14483fe01eb0a3ea7d1d46a30571">Musical Hexagons</a></i> is a two-dimensional spatial arrangement of the chromatic musical notes, written in JavaScript</p>
          </div>
        </div>
      
        <div class="one-half column">
          
         <p class="after-image"> Contrary to a more typical client project that starts with a clear problem, my <i>Rumble Fish</i> project, started with an idea for exploration—integrating biological patterns into generative music systems. I envisioned a system where the movements of goldfish would trigger notes in a two-dimensional grid of musical notes. An initial experiment with a JavaScript app called Musical Hexagons, quickly highlighted the limitations of my approach; it produced more nosiey randomness than any sense of harmony or coherence. This stumbling block prompted a pivot in my approach, pushing me into the deep end of the pool of generative music. My first goal then was to gain a much better understanding of generative music systems in general and see what options might be available for me to implement my system and produce a more compelling sound. 

         </p>
        </div>
      </div>

      <div class="row">
               <h4 class="after-image">Deep dive into understanding</h4>
               <div class="one-half column u-pull-right">
                <h3>"...accommodate many levels of listening attention without enforcing one in particular; it must be as ignorable as it is interesting."</h3>
              </div>
        <p>The world of ambient music and generative music systems is vast. A rich and very broad history, offered fertile ground for ideas. My research stumbled across many interesting tidbits - like Mussolini’s unexpected hand in the birth of Muzak!! But what really moved the needle were a couple of deep-dive, interactive articles by Tero Parviainen (@teropa). In particular his work provided me with the very important differentiation; a generative method for producing a piece of music will sound the same every time you play it. In contrast a generative product is a piece of music that is generated anew each time. In other words:

          
          <p><i>"What if we could, instead of making music, design systems that generate music for us?"</i></p>
         
         <p>The majority of projects I found with any relevance to <i>Rumble Fish</i> appeared to leverage Python, JavaScript, or a fusion of both, especially for the computer vision and object detection and tracking needed to capture fish movements. Due to my own distinct lack of programming skills, I moved towards Pure Data—an open-source visual programming language tailored for constructing interactive music and multimedia creations. This platform promised a more accessible entry point for me to weave together quick prototypes to explore the possibilities of Rumble Fish.
        </p>

          
      </div>

      <div class="row">
        <div class="one-half column">
          <img class="u-max-full-width" src="images/tero3.png">
        </div>

        <div class="one-half column">
          <img class="u-max-full-width" src="images/tero4.png">
        </div>

        <div class="caption">
          <p>Tero Parviainen has created two must-read, comprehensive interactive guides to generative music.</p>
        </div>
      </div>

       
       <div class="row">
        <h4 class="after-image">Filtering out a more defined problem</h4>
          
       </div>

       <div class="row">
        <div class="column">
         <p>With Rumble Fish I wanted to create a system that translated the random movements of a fish into an ambient music experience, blending the randomness of biology with the precision of digital. </p>
         
          <p>Distilling my key challenges I drafted the following brief:
          </p>
          <p><i>"Design a system that generates music by incorporating biological input from a goldfish's trajectory, capturing data to trigger sounds that create an audio experience that strikes a balance between reflecting the randomness of the fish movements and a compelling listening experience." </i></p>
       </div>


       <div class="row">
        <h4 class="after-image">Fishing for ideas</h4>
          
       </div>

       <div class="row">
        <div class="column">
         <p> I considered tracking speed, direction, or position as the potential variables for triggering sounds. I would then need to set up a system of musical notes, to somehow correspond to those tracked variables. Initially, I investigated the use of motion detection to correspond with triggers for various sounds. This proved to be very complex to implement. Later I thought about dividing the screen into several vertical segments that would correspond with different but related notes. This would simplify the tracking to object detection at a set intervals, a much simpler approach to triggering sounds. 
        </p>
       </div>


       <div class="row">
        <div class="one-half column">
          <img class="u-max-full-width" src="images/sketch3.png" width="540" height="270">
        </div>

        <div class="one-half column">
          <img class="u-max-full-width" src="images/sketch4.png" width="500" height="270">
        </div>

        
      </div>

       <div class="row">
        <div class="one-half column">
          <p><div class="caption">
            <p>These are rough early sketches on top, and an animated gif showing how we might trigger sound generators as Goldie is detected in different segments.</p>
          </div>
            </p>
        </div>

        <div class="one-half column">
          <img class="u-max-full-width" src="images/goldfish2.gif" width="500" height="270" >
        </div>
      </div>

      <div class="row">
        <h4 class="after-image">Testing ideas, again</h4>
          
       </div>

       <div class="row">
        <div class="one-half column">
          <p>Inspired by the insights from my research, especially the articles from Tero Parviainen I produced a system of six sound generators in Pure Data. Through trial and error with note arrangements and durations, and a several hours of YouTube videos (!) I managed to create a system that started to generate a more engaging sound. I then experimented, slightly less successfully with Pure Data to build an object detector. My challenges however,  sparked a move away from using consecutive segments to a matrix (2x4 or 2x3) with deeper/lower notes along the bottom and higher notes along the top. This would allow <i>Rumble Fish</i> to more clearly reflect the changes in the fish's trajectory without compromising the sound by over doing it with random changes.
            </p>
        </div>

        <div class="one-half column">
          <img class="u-max-full-width" src="images/pd1.png">
          <div class="caption">
            <p>This an example of a Pure Data patch that generates a note at a given frequency and duration.  </p>
            <p><a href="images/fishMusic.m4a">Listen!</a> </p>

          
          </div>
        </div>
      </div>

      


   
</div>

<div class="row">
  <div class="column">
    <h4>Rumble Fish: A Biological Music System Concept</h4>
        <p><i>Rumble Fish</i> will provide a unique auditory ecosystem that generates a tranquil soundscape reflecting the calmness of aquarium life. As a goldfish glides through the water, its movements are captured in real-time by a camera. The captured video feeds into an object detection module that tracks the goldfish trajectories, translating its motion into coordinates. These coordinates are matched with a corresponding segment in a matrix, each of which is associated with a unique sound generator. Each sound generator is assigned a specific musical note and length, and corresponds to different areas within the tank. There are six sound generators in total, that correspond to a 2x3 grid overlaid on the tank—deeper notes at the bottom, higher notes at the top. As the fish moves, it triggers the sound generators, which then send their individual notes to a digital audio converter. The converter blends these inputs into a cohesive, harmonious piece of ambient music, which can be experienced through headphones or speakers. 
        </p>
      </div>

  </div>
</div>

      <div class="row">
        <div class="column">
          <img class="u-max-full-width" src="images/concept1.png">
            </p>
            <div class="caption">
              <p>A biological music system concept</p>
            </div>
  
        </div>
      </div>

      <div class="row">
        <div class="column">
          <h4>Lessons learnt</h4>
              <p>This project gave me an opportunity to reflect on my design process, and offered a unique perspective on the design process, without the pressure of delivering for clients. Having the freedom to explore any area of emerging technology was intially very enticing. However, in some ways the lack of constraints ended up becoming the most challenging aspect. Without constraints other than a submission date, or an initial brief to anchor research, quite a lot of time was spend in ideation and trying to identify an area of interest that gave easy access to technology. The new Vision Pro from Apple for example looks very exciting but any development work or experimentation would require prehibitively expensive equipment and programming proficiency. Other technologies such as AI seem to have more open source development possibilities but in many cases require a significant proficiency in data science or Python programming.  
              </p>
              <p>Once I had a area to work on I realised that all my previously limited programming skills have evaporated, however chatGPT is incredibly impressive on this front. As a very novice programmer I spent much time trawling Stack Overflow for debugging solutions, whereas chatGPT streamlines and untangles much of that slog. My experience with it has been so encouraging I could see myself getting back into tinkering with technology on a more regular basis (once I finish this MSc!). </p>
              <p>I was very pleased with the outcome from Pure Data in terms of the ambient music compositions but would have loved to have had the time to figure out how to bridge what I perceive as a relatively small gap between the computer vision component for tracking and triggering the sound generators.  </p>
            </div>
      
        </div>
     


      <div class="row">
        <div class="column">
          <h4>Future rumblings</h4>
              <p>Further work is required to correctly tie the object detection to the sound generators to build a proof of concept. What might be particularly exciting would be to experiment with a neural network such as Google's Magenta to play around with completely new sounds, still driven by the fish movements. One idea might be to create a fish tank raga that subtly changes over the course of the day or take other contextual information into account to setup the parameters for the sound generators to better achieve the goal laid out by Eno in Music for Airports, and generating music that is "as ignorable as it is interesting".
              </p>
              <p></p><p></p><p></p>
            </div>
            <h4></h4>
        </div>

        <div class="row">
          <div class="column">
            <p></p><p></p>
            <img class="u-max-full-width" src="images/Rumblefish-Featured.jpg">
              </p>
              <div class="caption">
                <p>A hat tip to <a href="https://www.criterion.com/films/28993-rumble-fish">Francis Ford Coppola</a> for the name inspiration! </p>
              </div>
    
          </div>
        </div>



      </div>

     

      

       

         



    
      
    </div>
  </div>

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
